{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/eborin/SSL-course/blob/main/16_minerva_BYOL-STL10-backbone_pretrain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "[View Source Code](https://github.com/eborin/SSL-course/blob/main/16_minerva_BYOL-STL10-backbone_pretrain.ipynb)\n",
    "\n",
    "# Pretraining backbones with Minerva BYOL\n",
    "\n",
    "This notebook provides a demonstration of how to pretrain feature extraction backbones using the Minerva BYOL model. \n",
    "In particular, it walks through the process of training a ResNet-18 backbone on the \"unlabeled\" split of the STL10 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Objective\n",
    "\n",
    "The main objective of this tutorial is to present how to employ Minerva BYOL to pretrain a given backbone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Before Running this Notebook\n",
    "\n",
    "The training operation performed in this notebook is likely to take a considerable amount of time when executed on typical laptop or desktop hardware.\n",
    "As of May 2025, it also remains time-consuming even when running on Google Colab.\n",
    "If you have access to a system equipped with a powerful GPU, it is recommended that you run this notebook on that system to significantly reduce the training time.\n",
    "\n",
    "#### 1.1.1. Running this notebook from a terminal as a Python script.\n",
    "\n",
    "You can convert this notebook into a Python script by executing the following command in your terminal:\n",
    "\n",
    "```bash\n",
    "jupyter nbconvert --to script 16_minerva_BYOL-STL10-backbone_pretrain.ipynb\n",
    "```\n",
    "\n",
    "This will generate a Python script named `16_minerva_BYOL-STL10-backbone_pretrain.py`, which you can then run directly from your terminal using:\n",
    "\n",
    "```bash\n",
    "python 16_minerva_BYOL-STL10-backbone_pretrain.py\n",
    "```\n",
    "\n",
    "> **Note**: Before converting the notebook, you may want to adjust the main configuration variables found in the \"Basic Setup\" section to ensure they are appropriately set for your environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 BYOL\n",
    "\n",
    "BYOL (Bootstrap Your Own Latent) is a self-supervised learning framework introduced by DeepMind.\n",
    "It learns useful visual representations without requiring labeled data or negative pairs, by encouraging consistency between different augmented views of the same image using a bootstrapping mechanism.\n",
    "\n",
    "The method was presented in a paper published as a preprint on the [arXiv repository](https://arxiv.org/pdf/2006.07733), and has significantly influenced the development of non-contrastive self-supervised learning techniques since its release in 2020."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 What we're going to cover\n",
    "\n",
    "In this tutorial, we demonstrate how to use the BYOL model from the [Minerva framework](https://github.com/discovery-unicamp/Minerva) to train a ResNet-18 backbone network.\n",
    "Specifically, we will train a ResNet18-based backbone using the \"unlabeled\" split of the STL10 dataset.\n",
    "\n",
    "The training process closely follows the approach used in the `09_minerva_SimCLR-STL10-backbone_pretrain.ipynb` tutorial, but here we apply the BYOL self-supervised learning technique instead of SimCLR.\n",
    "\n",
    "| **Topic** | **Contents** |\n",
    "| ----- | ----- |\n",
    "| [**2. Basic Setup**](#sec_2) | Import useful modules (torch, torchvision, and lightning). |\n",
    "| [**3. Setting up the Dataset**](#sec_3) | Set up the data transforms, the dataset and the data module for the traininig process. |\n",
    "| [**4. Create the Model for the Pretext Task**](#sec_4) | Create the backbone, the projection head, and the model for the pretext task. |\n",
    "| [**5. Setting up the KNN benchmark**](#sec_5) | Create a benchmark to track the performance of the backbone on the downstream task during training. |\n",
    "| [**6. Training the model**](#sec_6) | Create the trainer object and invoke the `fit` method to train the model. |\n",
    "| [**7. Exercises**](#sec_7) | Suggested Exercises. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Where can you get help?\n",
    "\n",
    "In addition to discussing with your colleagues or the course professor, you might also consider:\n",
    "\n",
    "* Minerva: check the [Minerva docs](https://discovery-unicamp.github.io/Minerva/).\n",
    "\n",
    "* Lightning: check the [Lightning documentation](https://lightning.ai/docs/overview/getting-started) and research or post Lightning related question on the [PyTorch Lightning forum](https://lightning.ai/forums/).\n",
    "\n",
    "* PyTorch: check the [PyTorch documentation](https://pytorch.org/docs/stable/index.html) and research or post PyTorch related question on the [PyTorch developer forums](https://discuss.pytorch.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"sec_2\">2. Basic Setup</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Setup main variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several variables influence the execution of this notebook, particularly in terms of memory usage and training time. These include:\n",
    "\n",
    "* **`n_epochs`**: Specifies the maximum number of training epochs. \n",
    "    Increasing this value generally improves backbone performance but also leads to longer training times. \n",
    "    Reducing the number of epochs can speed up training, but doing so excessively may compromise the quality of the learned representations. \n",
    "    Based on my experiments, training for at least 90 epochs typically yields backbones with noticeably better performance compared to random backbones (You will be able to evaluate this in the next tutorials).\n",
    "\n",
    "* **`checkpoint_every_n_epochs`**: Specifies how often, in terms of training epochs, a model checkpoint is saved. \n",
    "    For example, if set to 10, the model's state will be saved every 10 epochs during training.\n",
    "    These checkpoints can be used to recover from interruptions, and they also allow you to evaluate the backbone's performance at various stages to monitor how the learned representations are evolving over time.\n",
    "\n",
    "* **`DL_BATCH_SIZE`**: Determines the batch size used during training. \n",
    "    Very large batches may exceed your GPU's memory capacity, however, small batch sizes may affect the quality of the representation learned by BYOL. Adjust accordingly based on available resources.\n",
    "\n",
    "* **`DL_NUM_WORKERS`**: Sets the number of worker threads used by the DataLoader for parallel data loading and preprocessing. \n",
    "    Increasing this value can help improve data throughput and reduce training bottlenecks, especially on multi-core systems.\n",
    "\n",
    "You can customize these parameters in the following code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of epochs for training the model using the BYOL pretext task.\n",
    "n_epochs = 200\n",
    "\n",
    "# Number of epochs between model checkpoints\n",
    "checkpoint_every_n_epochs = n_epochs // 10\n",
    "\n",
    "# Dataloaders/Datamodule parameters\n",
    "DL_BATCH_SIZE=256\n",
    "DL_NUM_WORKERS=16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Installing Lightining and Minerva modules\n",
    "\n",
    "The code below attempts to import the Minerva module and will automatically install it if it is not already available.\n",
    "> **Note**: Since Minerva depends on PyTorch Lightning, Lightning will also be installed automatically if it is not already present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import minerva\n",
    "except:\n",
    "    try:\n",
    "        #Try to install it and import again\n",
    "        print(\"[INFO]: Could not import the minerva module. Trying to install it!\")\n",
    "        !pip install -q minerva-ml\n",
    "        import minerva\n",
    "        print(\"[INFO]: It looks like minerva was successfully imported!\")\n",
    "    except:\n",
    "        raise Exception(\"[ERROR] Couldn't find the minerva module ... \\n\" +\n",
    "                        \"Please, install it before running the notebook.\\n\"+\n",
    "                        \"You might want to install the modules listed at requirements.txt\\n\" +\n",
    "                        \"To do so, run: \\\"pip install -r requirements.txt\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Importing basic modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the basic modules, such as lightning, torch, minerva, and other utility modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cu124\n",
      "torchvision version: 0.21.0+cu124\n",
      "Lightning version: 2.5.1\n"
     ]
    }
   ],
   "source": [
    "# Import PyTorch\n",
    "import torch\n",
    "\n",
    "# Import torchvision\n",
    "import torchvision\n",
    "\n",
    "# Import lightning\n",
    "import lightning\n",
    "\n",
    "# Import minerva\n",
    "import minerva\n",
    "\n",
    "# Check versions\n",
    "# Note: your PyTorch version shouldn't be lower than 1.10.0 and torchvision version shouldn't be lower than 0.11\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"torchvision version: {torchvision.__version__}\")\n",
    "print(f\"Lightning version: {lightning.__version__}\")\n",
    "#print(f\"Minerva version: {M.__version__}\") ## TODO\n",
    "\n",
    "# Import matplotlib for visualization\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"sec_3\">3. Setting up the Dataset</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the unlabeled split of the STL10 dataset to pretrain our backbone. \n",
    "To enable contrastive learning, we will apply a series of data transformations to generate randomly augmented views of each image.\n",
    "\n",
    "For a detailed discussion of the data augmentation strategies used in the next code block, please refer to the tutorial:\n",
    "`08_minerva_data_transforms.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torchvision transforms\n",
    "from torchvision.transforms.v2 import Compose, ToImage, ToDtype, RandomHorizontalFlip, RandomResizedCrop, RandomApply, ColorJitter, RandomGrayscale, GaussianBlur, Normalize\n",
    "# Minerva Contrastive transform\n",
    "from minerva.transforms.transform import ContrastiveTransform\n",
    "\n",
    "# STL10 statistics for the unlabeled split. \n",
    "# - Note: If you would like to compute these statistics for your own dataset, refer \n",
    "#         to the discussion in tutorial 05_pytorch_transfer_learning.ipynb.\n",
    "stl10_unlabeled_mean  = torch.tensor([0.4406, 0.4273, 0.3858])\n",
    "stl10_unlabeled_std = torch.tensor([0.2687, 0.2613, 0.2685])\n",
    "\n",
    "transform_pipeline = Compose([\n",
    "    ToImage(), \n",
    "    ToDtype(torch.float32, scale=True),\n",
    "    RandomHorizontalFlip(),\n",
    "    RandomResizedCrop(size=96),\n",
    "    RandomApply([ColorJitter(brightness=0.5,contrast=0.5,saturation=0.5,hue=0.1)], p=0.8),\n",
    "    RandomGrayscale(p=0.2),\n",
    "    GaussianBlur(kernel_size=9),\n",
    "    Normalize(mean=stl10_unlabeled_mean, std=stl10_unlabeled_std)\n",
    "])\n",
    "\n",
    "contrastive_transform = ContrastiveTransform(transform_pipeline)\n",
    "\n",
    "contrastive_dataset = torchvision.datasets.STL10(root=\"data\", split=\"unlabeled\",  download=True,\n",
    "                                                 transform=contrastive_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BYOL SSL class expects the dataloader to return batches containing an array with two augmented versions of each input sample.\n",
    "Unlike SimCLR, it does not expect a tuple with the sample features and label. \n",
    "Therefore, we must modify the dataloader to return only the input data.\n",
    "To achieve this, we define a simple utility class called `FeatureOnlyDataset`, which ensures that only the input features are retrieved when the `__getitem__` method is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturesOnlyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, original_dataset):\n",
    "        self.dataset = original_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        features, _ = self.dataset[idx]\n",
    "        return features\n",
    "\n",
    "contrastive_dataset_features_only = FeaturesOnlyDataset(contrastive_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To monitor learning performance, we will split the dataset into 80% training and 20% validation subsets.\n",
    "Additionally, we will use a `MinervaDataModule` to streamline data handling and simplify the training workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "from minerva.data.data_modules.base import MinervaDataModule\n",
    "\n",
    "torch.manual_seed(42)\n",
    "train_size = int(0.8 * len(contrastive_dataset_features_only))\n",
    "val_size = len(contrastive_dataset_features_only) - train_size\n",
    "train_set, val_set = random_split(contrastive_dataset_features_only, [train_size, val_size])\n",
    "\n",
    "datamodule = MinervaDataModule(name=\"Contrastive STL10\",\n",
    "                                train_dataset=train_set, \n",
    "                                val_dataset=val_set, \n",
    "                                test_dataset=None,\n",
    "                                batch_size=DL_BATCH_SIZE, \n",
    "                                num_workers=DL_NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"sec_4\">4. Create the Model for the Pretext Task</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Backbone and Projection Head Generation\n",
    "\n",
    "We will use a modified version of the ResNet18 model as the backbone. \n",
    "Specifically, we replace its final fully connected (fc) layer with an identity layer—`torch.nn.Identity()`—which effectively removes any operation at that stage, allowing us to extract raw feature representations.\n",
    "\n",
    "The `generate_backbone()` function handles this process: it instantiates a ResNet18 model, replaces its fully connected layer with an identity layer, and returns the modified model.\n",
    "\n",
    "In the following code block, we instantiate the backbone and display its architecture using the summary() function from the torchinfo package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "========================================================================================================================\n",
       "Layer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable\n",
       "========================================================================================================================\n",
       "ResNet (ResNet)                          [32, 3, 96, 96]      [32, 512]            --                   True\n",
       "├─Conv2d (conv1)                         [32, 3, 96, 96]      [32, 64, 48, 48]     9,408                True\n",
       "├─BatchNorm2d (bn1)                      [32, 64, 48, 48]     [32, 64, 48, 48]     128                  True\n",
       "├─ReLU (relu)                            [32, 64, 48, 48]     [32, 64, 48, 48]     --                   --\n",
       "├─MaxPool2d (maxpool)                    [32, 64, 48, 48]     [32, 64, 24, 24]     --                   --\n",
       "├─Sequential (layer1)                    [32, 64, 24, 24]     [32, 64, 24, 24]     --                   True\n",
       "│    └─BasicBlock (0)                    [32, 64, 24, 24]     [32, 64, 24, 24]     --                   True\n",
       "│    │    └─Conv2d (conv1)               [32, 64, 24, 24]     [32, 64, 24, 24]     36,864               True\n",
       "│    │    └─BatchNorm2d (bn1)            [32, 64, 24, 24]     [32, 64, 24, 24]     128                  True\n",
       "│    │    └─ReLU (relu)                  [32, 64, 24, 24]     [32, 64, 24, 24]     --                   --\n",
       "│    │    └─Conv2d (conv2)               [32, 64, 24, 24]     [32, 64, 24, 24]     36,864               True\n",
       "│    │    └─BatchNorm2d (bn2)            [32, 64, 24, 24]     [32, 64, 24, 24]     128                  True\n",
       "│    │    └─ReLU (relu)                  [32, 64, 24, 24]     [32, 64, 24, 24]     --                   --\n",
       "│    └─BasicBlock (1)                    [32, 64, 24, 24]     [32, 64, 24, 24]     --                   True\n",
       "│    │    └─Conv2d (conv1)               [32, 64, 24, 24]     [32, 64, 24, 24]     36,864               True\n",
       "│    │    └─BatchNorm2d (bn1)            [32, 64, 24, 24]     [32, 64, 24, 24]     128                  True\n",
       "│    │    └─ReLU (relu)                  [32, 64, 24, 24]     [32, 64, 24, 24]     --                   --\n",
       "│    │    └─Conv2d (conv2)               [32, 64, 24, 24]     [32, 64, 24, 24]     36,864               True\n",
       "│    │    └─BatchNorm2d (bn2)            [32, 64, 24, 24]     [32, 64, 24, 24]     128                  True\n",
       "│    │    └─ReLU (relu)                  [32, 64, 24, 24]     [32, 64, 24, 24]     --                   --\n",
       "├─Sequential (layer2)                    [32, 64, 24, 24]     [32, 128, 12, 12]    --                   True\n",
       "│    └─BasicBlock (0)                    [32, 64, 24, 24]     [32, 128, 12, 12]    --                   True\n",
       "│    │    └─Conv2d (conv1)               [32, 64, 24, 24]     [32, 128, 12, 12]    73,728               True\n",
       "│    │    └─BatchNorm2d (bn1)            [32, 128, 12, 12]    [32, 128, 12, 12]    256                  True\n",
       "│    │    └─ReLU (relu)                  [32, 128, 12, 12]    [32, 128, 12, 12]    --                   --\n",
       "│    │    └─Conv2d (conv2)               [32, 128, 12, 12]    [32, 128, 12, 12]    147,456              True\n",
       "│    │    └─BatchNorm2d (bn2)            [32, 128, 12, 12]    [32, 128, 12, 12]    256                  True\n",
       "│    │    └─Sequential (downsample)      [32, 64, 24, 24]     [32, 128, 12, 12]    8,448                True\n",
       "│    │    └─ReLU (relu)                  [32, 128, 12, 12]    [32, 128, 12, 12]    --                   --\n",
       "│    └─BasicBlock (1)                    [32, 128, 12, 12]    [32, 128, 12, 12]    --                   True\n",
       "│    │    └─Conv2d (conv1)               [32, 128, 12, 12]    [32, 128, 12, 12]    147,456              True\n",
       "│    │    └─BatchNorm2d (bn1)            [32, 128, 12, 12]    [32, 128, 12, 12]    256                  True\n",
       "│    │    └─ReLU (relu)                  [32, 128, 12, 12]    [32, 128, 12, 12]    --                   --\n",
       "│    │    └─Conv2d (conv2)               [32, 128, 12, 12]    [32, 128, 12, 12]    147,456              True\n",
       "│    │    └─BatchNorm2d (bn2)            [32, 128, 12, 12]    [32, 128, 12, 12]    256                  True\n",
       "│    │    └─ReLU (relu)                  [32, 128, 12, 12]    [32, 128, 12, 12]    --                   --\n",
       "├─Sequential (layer3)                    [32, 128, 12, 12]    [32, 256, 6, 6]      --                   True\n",
       "│    └─BasicBlock (0)                    [32, 128, 12, 12]    [32, 256, 6, 6]      --                   True\n",
       "│    │    └─Conv2d (conv1)               [32, 128, 12, 12]    [32, 256, 6, 6]      294,912              True\n",
       "│    │    └─BatchNorm2d (bn1)            [32, 256, 6, 6]      [32, 256, 6, 6]      512                  True\n",
       "│    │    └─ReLU (relu)                  [32, 256, 6, 6]      [32, 256, 6, 6]      --                   --\n",
       "│    │    └─Conv2d (conv2)               [32, 256, 6, 6]      [32, 256, 6, 6]      589,824              True\n",
       "│    │    └─BatchNorm2d (bn2)            [32, 256, 6, 6]      [32, 256, 6, 6]      512                  True\n",
       "│    │    └─Sequential (downsample)      [32, 128, 12, 12]    [32, 256, 6, 6]      33,280               True\n",
       "│    │    └─ReLU (relu)                  [32, 256, 6, 6]      [32, 256, 6, 6]      --                   --\n",
       "│    └─BasicBlock (1)                    [32, 256, 6, 6]      [32, 256, 6, 6]      --                   True\n",
       "│    │    └─Conv2d (conv1)               [32, 256, 6, 6]      [32, 256, 6, 6]      589,824              True\n",
       "│    │    └─BatchNorm2d (bn1)            [32, 256, 6, 6]      [32, 256, 6, 6]      512                  True\n",
       "│    │    └─ReLU (relu)                  [32, 256, 6, 6]      [32, 256, 6, 6]      --                   --\n",
       "│    │    └─Conv2d (conv2)               [32, 256, 6, 6]      [32, 256, 6, 6]      589,824              True\n",
       "│    │    └─BatchNorm2d (bn2)            [32, 256, 6, 6]      [32, 256, 6, 6]      512                  True\n",
       "│    │    └─ReLU (relu)                  [32, 256, 6, 6]      [32, 256, 6, 6]      --                   --\n",
       "├─Sequential (layer4)                    [32, 256, 6, 6]      [32, 512, 3, 3]      --                   True\n",
       "│    └─BasicBlock (0)                    [32, 256, 6, 6]      [32, 512, 3, 3]      --                   True\n",
       "│    │    └─Conv2d (conv1)               [32, 256, 6, 6]      [32, 512, 3, 3]      1,179,648            True\n",
       "│    │    └─BatchNorm2d (bn1)            [32, 512, 3, 3]      [32, 512, 3, 3]      1,024                True\n",
       "│    │    └─ReLU (relu)                  [32, 512, 3, 3]      [32, 512, 3, 3]      --                   --\n",
       "│    │    └─Conv2d (conv2)               [32, 512, 3, 3]      [32, 512, 3, 3]      2,359,296            True\n",
       "│    │    └─BatchNorm2d (bn2)            [32, 512, 3, 3]      [32, 512, 3, 3]      1,024                True\n",
       "│    │    └─Sequential (downsample)      [32, 256, 6, 6]      [32, 512, 3, 3]      132,096              True\n",
       "│    │    └─ReLU (relu)                  [32, 512, 3, 3]      [32, 512, 3, 3]      --                   --\n",
       "│    └─BasicBlock (1)                    [32, 512, 3, 3]      [32, 512, 3, 3]      --                   True\n",
       "│    │    └─Conv2d (conv1)               [32, 512, 3, 3]      [32, 512, 3, 3]      2,359,296            True\n",
       "│    │    └─BatchNorm2d (bn1)            [32, 512, 3, 3]      [32, 512, 3, 3]      1,024                True\n",
       "│    │    └─ReLU (relu)                  [32, 512, 3, 3]      [32, 512, 3, 3]      --                   --\n",
       "│    │    └─Conv2d (conv2)               [32, 512, 3, 3]      [32, 512, 3, 3]      2,359,296            True\n",
       "│    │    └─BatchNorm2d (bn2)            [32, 512, 3, 3]      [32, 512, 3, 3]      1,024                True\n",
       "│    │    └─ReLU (relu)                  [32, 512, 3, 3]      [32, 512, 3, 3]      --                   --\n",
       "├─AdaptiveAvgPool2d (avgpool)            [32, 512, 3, 3]      [32, 512, 1, 1]      --                   --\n",
       "├─Identity (fc)                          [32, 512]            [32, 512]            --                   --\n",
       "========================================================================================================================\n",
       "Total params: 11,176,512\n",
       "Trainable params: 11,176,512\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 10.66\n",
       "========================================================================================================================\n",
       "Input size (MB): 3.54\n",
       "Forward/backward pass size (MB): 233.57\n",
       "Params size (MB): 44.71\n",
       "Estimated Total Size (MB): 281.82\n",
       "========================================================================================================================"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "# Function to generate a ResNet18 based backbone.\n",
    "def generate_backbone(weights=None):\n",
    "    backbone = resnet18(weights=weights)\n",
    "    backbone.fc = torch.nn.Identity()\n",
    "    return backbone\n",
    "\n",
    "# Generate the backbone and check its structure\n",
    "backbone = generate_backbone()\n",
    "\n",
    "summary(backbone,\n",
    "        input_size=(32, 3, 96, 96), # input data shape (N x C x H x W)\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the output of the backbone is a 512-dimensional feature vector, as indicated by the Identity (fc) layer in the model summary.\n",
    "\n",
    "Next, we will define the projection head—the component that will be attached to the backbone to form the complete pretext model used during contrastive learning.\n",
    "\n",
    "For this purpose, we’ll use a simple multi-layer perceptron (MLP), defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from minerva.models.nets.mlp import MLP\n",
    "import torch\n",
    "\n",
    "def generate_BYOL_proj_head(input_dim=512, output_dim=256):\n",
    "        return MLP(\n",
    "                layer_sizes=[input_dim, 4096, output_dim],\n",
    "                activation_cls=torch.nn.ReLU,\n",
    "                intermediate_ops=[torch.nn.BatchNorm1d(4096), None],\n",
    "               )\n",
    "\n",
    "def generate_BYOL_pred_head(input_dim=256, output_dim=256):\n",
    "    \"\"\"Creates the default prediction head used in BYOL.\"\"\"\n",
    "    return MLP(\n",
    "        layer_sizes=[input_dim, 4096, output_dim],\n",
    "        activation_cls=torch.nn.ReLU,\n",
    "        intermediate_ops=[torch.nn.BatchNorm1d(4096), None],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Adjusting the BYOL optimizer\n",
    "\n",
    "For our experiments, we will employ PyTorch's `Adam` optimizer with learning rate = 0.0001 and weight decay = 1e-6.\n",
    "\n",
    "To apply this configuration, we need to modify the `configure_optimizers()` method of the pretext model so that it returns our chosen optimizer and scheduler.\n",
    "\n",
    "While one clean approach would be to extend the Minerva BYOL class, we will opt for a quicker, more pragmatic solution: we will monkey-patch the `configure_optimizers` method directly on the model instance. This is handled by the `adjust_configure_optimizers_Adam()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_configure_optimizers_Adam(model, \n",
    "                                lr=1e-4, # Adam Optimizer learning rate parameter\n",
    "                                weight_decay=1e-6): # Adam Optimizer weight_decay parameter\n",
    "    # Redefine the optimizers\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    model.configure_optimizers = configure_optimizers.__get__(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Building and Configuring the Model for the Pretext Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build and configure the model for the pretext task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from minerva.models.ssl.byol import BYOL\n",
    "\n",
    "# Create the backbone and the projection head\n",
    "backbone  = generate_backbone()\n",
    "BYOL_proj_head = generate_BYOL_proj_head()\n",
    "BYOL_pred_head = generate_BYOL_pred_head()\n",
    "\n",
    "# Create the model for the pretext task\n",
    "BYOL_model = BYOL(backbone=backbone, \n",
    "                  projection_head=BYOL_proj_head, \n",
    "                  prediction_head=BYOL_pred_head)\n",
    "\n",
    "# # Adjusting the model optimizers\n",
    "adjust_configure_optimizers_Adam(BYOL_model, lr=0.0001, weight_decay=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"sec_5\">5. Setting up the KNN benchmark</a>\n",
    "\n",
    "In this section, we will create a benchmark and attach it to the trainer object to track the backbone's performance on the downstream task when training with BYOL.\n",
    "Specifically, at the end of each epoch, we will use the backbone to encode the target dataset features and train and evaluate a KNN model with these encoded features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Implementing the `KNN_Benchmark` class\n",
    "\n",
    "Our KNN benchmark will be implemented as a PyTorch Lightning Callback that is invoked by the trainer at the end of each training epoch.\n",
    "To achieve this, we will define a class that extends the `pytorch_lightning.callbacks.Callback` class and override the `on_train_epoch_end()` method.\n",
    "For example:\n",
    "\n",
    "```python\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "\n",
    "class KNNBenchmark(Callback):\n",
    "    def __init__(self, ...):\n",
    "        # Initialization code\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, model):\n",
    "        # Benchmark evaluation code\n",
    "```\n",
    "\n",
    "After defining the class, we create a benchmark object and attach it to the trainer in the same way we attach other callbacks, such as `ModelCheckpoint` and `LearningRateMonitor`.\n",
    "\n",
    "With this setup, the `on_train_epoch_end(self, trainer, model)` method will be automatically invoked by the trainer at the end of every epoch, allowing us to extract features from the benchmark dataset using the model's backbone and evaluate a KNN model on these features.\n",
    "\n",
    "To simplify the process, we will employ `scikit-learn` `KNeighborsClassifier` class to implement the KNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn import functional as F\n",
    "from lightning.pytorch.callbacks import Callback\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class KNN_Benchmark(Callback):\n",
    "    \"\"\"\n",
    "    KNN Benchmark that can be attached to the Lightning trainer object to track the backbone's performance on the downstream task.\n",
    "    It expects the PyTorch Lightning model to have a backbone attribute.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, train_dataset, test_dataset, K: int = 10) -> None:\n",
    "        \"\"\"        \n",
    "        Args:\n",
    "            train_dataset: downstream training dataset -- N samples with D features - D must be compatible with the backbone.\n",
    "            test_dataset: downstream test dataset -- M samples with D features - D must be compatible with the backbone.\n",
    "            K: hyperparameter for KNN\n",
    "\n",
    "        The callback will be invoked at the end of every epoch to compute the accuracy using KNN on the features extracted by the backbone of the lightning model.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Store the features and labels from the downstream train and test datasets\n",
    "        self.train_X, self.train_y = self.dataset_to_tensors(train_dataset)\n",
    "        self.test_X, self.test_y = self.dataset_to_tensors(test_dataset)\n",
    "\n",
    "        # Set the KNN hyperparameter\n",
    "        self.K = K\n",
    "\n",
    "        # Create the KNN classifier\n",
    "        self.skl_KNN = KNeighborsClassifier(n_neighbors=self.K)    \n",
    "\n",
    "    def on_train_epoch_end(self, trainer, model):\n",
    "        # Use the model backbone to compute the features.\n",
    "        train_features, test_features = self.compute_features(model.backbone)\n",
    "        # Train the KNN model with the train set\n",
    "        self.skl_KNN.fit(train_features, self.train_y)\n",
    "        # Predict and compute the accuracy with the test set\n",
    "        y_pred = self.skl_KNN.predict(test_features)\n",
    "        acc = accuracy_score(self.test_y, y_pred)\n",
    "        # Log the result using the PyTorch Lightning model logger.\n",
    "        model.log(\"KNN_acc\", acc, on_step=False, on_epoch=True, sync_dist=True)\n",
    "\n",
    "    # Organize the features and labels from the dataset samples into two tensors.\n",
    "    def dataset_to_tensors(self, dataset):\n",
    "        features_l = [ f for f,l in dataset ]\n",
    "        labels_l = [ l for f,l in dataset ]\n",
    "        return torch.stack(features_l), torch.tensor(labels_l)\n",
    "\n",
    "    # Extract the features using the backbone\n",
    "    def compute_features(self, backbone):\n",
    "        backbone_device = next(backbone.parameters()).device\n",
    "        with torch.no_grad():\n",
    "            # Extract features from the train and test datasets using the model backbone\n",
    "            train_features = backbone( self.train_X.to(backbone_device) ).flatten(start_dim=1)\n",
    "            test_features = backbone( self.test_X.to(backbone_device) ).flatten(start_dim=1)\n",
    "        return train_features.to(\"cpu\"), test_features.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Setting Up the Downstream Dataset\n",
    "\n",
    "We will download the STL10 training dataset and split it into separate training and testing subsets.\n",
    "> Note: We will not use the STL10 test partition, as we want to avoid biasing our decisions based on the official test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torchvision transforms\n",
    "from torchvision.transforms.v2 import Compose, ToImage, ToDtype, Normalize\n",
    "\n",
    "# STL10 statistics for the train split\n",
    "stl10_train_mean = torch.tensor([0.4467, 0.4398, 0.4066])\n",
    "stl10_train_std  = torch.tensor([0.2603, 0.2566, 0.2713])\n",
    "\n",
    "# Build the data transform pipeline to convert from PIL images to tensors and normalize the data. \n",
    "transform_pipeline = Compose([\n",
    "    ToImage(), \n",
    "    ToDtype(torch.float32, scale=True),\n",
    "    Normalize(mean=stl10_train_mean, std=stl10_train_std)\n",
    "])\n",
    "\n",
    "# Build the dataset object (This step will download the dataset if it hasn't been previously downloaded).\n",
    "train_dataset = torchvision.datasets.STL10(root=\"data\", \n",
    "                                           split=\"train\",  \n",
    "                                           download=True,\n",
    "                                           transform=transform_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code split the train_dataset into train and validation subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# Split the data\n",
    "torch.manual_seed(42)\n",
    "train_size = int(0.80 * len(train_dataset))\n",
    "test_size = len(train_dataset) - train_size\n",
    "train_set, val_set = random_split(train_dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Create the Downstream Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "downstream_benchmark = KNN_Benchmark(train_dataset=train_set, \n",
    "                                     test_dataset=val_set,\n",
    "                                     K=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's call the `trainer.fit()` method to begin training the backbone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the training process finishes, you will find all the model weights in the `${log_ckpt_dir}/version_0/checkpoints/` directory, where `log_ckpt_dir` corresponds to the directory set in the previous code blocks.\n",
    "\n",
    "> **Note**: If `monitor_backbone_performance_with_downstream_benchmark` is set to `True`, the training process in the previous block will be skipped. \n",
    "    Instead, training will occur in the following section, where the trainer is configured with a callback to evaluate the backbone's performance on a designated downstream task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"sec_6\">6. Training the model</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we’ll use a PyTorch Lightning `Trainer` object to handle the training process.\n",
    "\n",
    "This time, however, we will enhance the trainer by adding several callbacks:\n",
    "\n",
    "* **`ModelCheckpoint`**: to save model weights at regular intervals and also store the best-performing weights (based on the lowest validation loss).\n",
    "  - We will also set `save_weights_only=True` to save only the model parameters (i.e., the backbone and projection head), excluding additional training states such as optimizer values and scheduler status.\n",
    "\n",
    "* **`LearningRateMonitor`**: to track and log the learning rate throughout training.\n",
    "\n",
    "* **`downstream_benchmark`**: The KNN benchmark to be executed at the end of every epoch. The goal is to assess whether the backbone's evolution is improving the feature representations for the downstream task, enabling a simple machine learning model to achieve better performance when trained on these features. It was defined in the previous section.\n",
    "\n",
    "Additionally, we will configure a `TensorBoardLogger` to log training metrics and store model checkpoints in the `logs/16_BYOL_STL/Pretext/BYOL-Resnet18/version_0/checkpoints/epoch=N-step=...ckpt` directory, where `N` corresponds to the number of epochs in which the checkpoint was saved.\n",
    "\n",
    "The following code sets up the trainer along with these callbacks and logging configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "from lightning import Trainer\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "log_ckpt_dir=f\"logs/16_BYOL_STL/Pretext\"\n",
    "trainer = Trainer(max_epochs=n_epochs,\n",
    "                  log_every_n_steps=16,\n",
    "                  benchmark=True,\n",
    "                  callbacks=[ModelCheckpoint(save_weights_only=True, mode='min', monitor='val_loss', save_last=\"link\"), \n",
    "                             ModelCheckpoint(save_weights_only=True, every_n_epochs=checkpoint_every_n_epochs, save_top_k=-1), \n",
    "                             LearningRateMonitor('epoch'), \n",
    "                             downstream_benchmark],\n",
    "                  logger = TensorBoardLogger(save_dir=log_ckpt_dir, name=f\"BYOL-Resnet18\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we invoke the trainer object with the BYOL (pretext) model and the contrastive version of the STL datamodule. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/configuration_validator.py:68: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
      "You are using a CUDA device ('NVIDIA A100 80GB PCIe') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                     | Type                     | Params | Mode \n",
      "------------------------------------------------------------------------------\n",
      "0 | backbone                 | ResNet                   | 11.2 M | train\n",
      "1 | projection_head          | MLP                      | 3.2 M  | train\n",
      "2 | prediction_head          | MLP                      | 2.1 M  | train\n",
      "3 | backbone_momentum        | ResNet                   | 11.2 M | train\n",
      "4 | projection_head_momentum | MLP                      | 3.2 M  | train\n",
      "5 | criterion                | NegativeCosineSimilarity | 0      | train\n",
      "------------------------------------------------------------------------------\n",
      "16.4 M    Trainable params\n",
      "14.3 M    Non-trainable params\n",
      "30.8 M    Total params\n",
      "123.117   Total estimated model params size (MB)\n",
      "155       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 313/313 [00:50<00:00,  6.15it/s, v_num=5, train_loss=-0.751]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/callbacks/model_checkpoint.py:384: `ModelCheckpoint(monitor='val_loss')` could not find the monitored key in the returned metrics: ['lr-Adam', 'train_loss', 'KNN_acc', 'epoch', 'step']. HINT: Did you call `log('val_loss', value)` in the `LightningModule`?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 313/313 [00:45<00:00,  6.82it/s, v_num=5, train_loss=-0.818]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 313/313 [00:47<00:00,  6.61it/s, v_num=5, train_loss=-0.818]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(BYOL_model, datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the training is complete, you will find several checkpoints at the `log_ckpt_dir/BYOL-Resnet18/` folder. \n",
    "These checkpoints were registered at different epochs/training steps.\n",
    "The `last.ckpt` is a link that points to the checkpoint that achieved the best validation loss.\n",
    "\n",
    "Now, we can use these checkpoins to load backbone weights and employ pre-trained backbones on downstream tasks -- this is the subject for another tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"sec_7\">7. Exercises</a>\n",
    "\n",
    "1) **Transformation Analysis**: Experiment with different combinations of data augmentation transforms and evaluate their impact on the performance of the Downstream Benchmark.\n",
    "\n",
    "2) **Projection Head Ablation**: Modify the projection head parameters and evaluate how these changes affect the quality of the learned representations.\n",
    "\n",
    "3) **Backbone Evaluation with BYOL**: Modify the tutorial `10_minerva_SimCLR-STL10-downstream_task.ipynb` to load and evaluate backbones pretrained using BYOL.\n",
    "    > **Hint**: Extend the notebook to allow a side-by-side comparison between the performance of backbones trained with SimCLR and BYOL.\n",
    "\n",
    "4) **Latent Space Visualization**: Adapt the `tutorial 12_minerva_SimCLR-STL10-latend_space_vis.ipynb` to visualize the latent spaces generated by BYOL-pretrained models.\n",
    "\n",
    "5) **Optimizer Comparison**: Update the training code to use the LARS optimizer with the same hyperparameters reported in the original BYOL paper. \n",
    "   Compare the downstream performance against the results obtained using the current optimizer.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
